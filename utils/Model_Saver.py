"""

Saverクラスの設計仕様

1. Saverクラスの主な目的:
   学習プロセス中に生成される重要なデータ（モデルパラメータ、学習ログ、エージェントの状態など）を
   永続化（ファイルに保存）すること．これにより、学習の中断・再開や、学習結果の分析を可能にする．

2. Saverクラスが担当する具体的なタスク:
   - モデルパラメータの保存: 各エージェントの学習済みモデルパラメータ（例: Q値テーブルやニューラルネットワークの重み）をファイルに保存する．
     IQLの場合はエージェントごとに、CQLの場合は共通のパラメータを保存する必要がある．
     ファイル形式はCSVなどを想定．
   - 学習ログの保存: 各エピソードにおける報酬、ステップ数、損失などの学習の進捗に関する情報をファイルに保存する．
     通常はCSV形式で、エピソードごとに追記していく形式を想定．
   - エージェントの状態履歴の保存（オプション）: 学習中にエージェントが到達した状態の履歴を保存する．
     これはヒートマップ描画などの分析に利用できる．CSV形式を想定．
   - 保存先のディレクトリ構造の管理: 学習設定（mask, reward_mode, 環境サイズなど）に基づいた適切なディレクトリパスを生成し、
     必要であればディレクトリを作成する．
   - ファイルパスの生成: 保存するファイル（モデルファイル、ログファイル、状態ファイルなど）のパスを生成する．

3. Saverクラスが担当しないタスク:
   - 環境との相互作用: 環境のリセットやステップ実行などの環境シミュレーションは行わない．
   - エージェントの行動選択や学習処理: エージェントの行動決定ロジックや、Q値/ネットワークの更新などの学習アルゴリズムは担当しない．
   - 学習のメインループ制御: エピソードの実行やステップの進行を制御するメインループは担当しない．
   - 学習結果のプロットや可視化: 保存されたデータを読み込んでグラフを作成する処理は担当しない（これはPlotResultsのような別のクラスが担当）．

4. Saverクラスが他のクラスとどのように連携するか:
   - MultiAgent_Qクラス:
     - MultiAgent_Qは学習のメインループを管理しており、エピソードの終了時や一定間隔でSaverクラスのメソッドを呼び出し、
       モデルパラメータや学習ログの保存を指示する．
     - MultiAgent_Qは学習設定（mask, reward_modeなど）やファイルパスに関する情報をSaverクラスに渡す．
     - Saverクラスは、MultiAgent_Qクラスが持つエージェントインスタンスや学習に関するデータ（報酬、損失、ステップ数など）を
       受け取り、ファイルに書き込む処理を行う．
   - Agent_Qクラス:
     - Saverクラスがモデルパラメータを保存する際、Agent_Qクラスが保持する学習パラメータ（例: Qテーブルやネットワークパラメータ）に
       アクセスする必要がある．Agent_QクラスはこれらのパラメータをSaverクラスからアクセス可能な形で提供する必要がある（例えば、
       パラメータを取得するメソッドをAgent_Qに実装する）．
     - Agent_Qは自身の学習に関する情報（例: 損失）をMultiAgent_Qに渡し、MultiAgent_QがSaverにログ保存を依頼する．
   - GridWorldクラス:
     - 直接的な連携はない．SaverクラスはGridWorldの状態（エージェントやゴールの位置）を保存することがあるが、
       それはMultiAgent_Q経由で受け取った状態データを利用する形になる．

"""